https://arxiv.org/pdf/2402.18039.pdf

作者在提出 method 之前做了很多“防止 LoRA 反向传播过程中梯度消失/爆炸” 的描述，目前我不知道这些是不是杞人忧天

三种做法：

- 和 ResNet 一样，将上一层的 input 加到这层的 input 上

- 将前**几**层 FFN 的 LoRA 加到当前层的的 LoRA 上

- 注意到 LoRA 是 BA 去乘 x，所以可以把前几层的 Bx 加到当前层的结果上再和 A 相乘

那么你自然就要问一些问题，比如这是怎么和原 LLM 合并的？第二个方法最简单，直接把这些 LoRA 的乘积算出来加到矩阵上就行了。

剩下两个基于一个假设即 $x_n = \alpha x_{n-1}$，但是我目前不清楚为什么 FFN 会有这种效果