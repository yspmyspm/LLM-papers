每篇文章读完之后需要详细写一下做法，这样如果后面记忆模糊之后可以快 速回溯一下，不用再翻原文了


# 增强可以处理的上下文范围

https://arxiv.org/pdf/2310.08560.pdf Towards LLMs as operation systems

# instruction tuning

https://github.com/SinclairCoder/Instruction-Tuning-Papers 这个 repository 收录了大量 instruction tuning 的论文。

# Mixture of experts 

这个 domain 还是比较重要的，应当快快花点时间研究一下

https://zhuanlan.zhihu.com/p/542465517 标题：经典论文一览

https://arxiv.org/pdf/2305.14705.pdf Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models

# parameter-efficient fine tuning

## LoRA 性能增强

https://arxiv.org/pdf/2402.18865.pdf interpolation based LoRA

https://arxiv.org/pdf/2402.07148.pdf X-LoRA

https://arxiv.org/pdf/2401.11504.pdf Temp-LoRA


# inference time training

https://arxiv.org/pdf/2310.13807.pdf Learn to (learn at test time)



# 未分类论文

https://arxiv.org/pdf/2402.15960.pdf 资源有限场景下，规划工具使用

https://arxiv.org/pdf/2310.15724.pdf

https://arxiv.org/pdf/2305.18390.pdf

https://arxiv.org/pdf/2210.09261.pdf

https://arxiv.org/pdf/1710.03937.pdf

https://arxiv.org/pdf/2104.02395.pdf

https://arxiv.org/pdf/2302.05738.pdf

file:///home/yspm/Downloads/2403.04931.pdf

https://arxiv.org/pdf/2306.02707.pdf orca-1

https://arxiv.org/pdf/2311.11045.pdf orca-2

https://arxiv.org/pdf/2402.14830.pdf orca-math 这三篇似乎是一串。

# 一些其它的网站

这些论文都比较老，早于 2023

https://random.jiayipan.me/posts/llm-core-papers/ ucb phd candidate blog
	
https://2023.aclweb.org/program/best_papers/ ACL2023 best papers

https://yynnyy.cn/ 清华大学 叶奕宁博客